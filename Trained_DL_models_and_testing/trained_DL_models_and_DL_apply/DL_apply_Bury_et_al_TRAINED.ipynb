{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c442c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Apply 500 length trained DL algorithm by Bury et. al. to test models.\n",
    "Edited from published codes by Bury et al. (2021), Deep learning for early warning signals of tipping points, PNAS. \n",
    "\n",
    "'''\n",
    "\n",
    "#python libraries\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "random.seed(datetime.now())\n",
    "\n",
    "\n",
    "if not os.path.exists('../data/ml_pred_Bury'):\n",
    "    os.makedirs('../data/ml_pred_Bury') \n",
    "\n",
    "if not os.path.exists('predictions'):\n",
    "    os.makedirs('predictions')\n",
    "\n",
    "type = 'forced'                                             #types are 'forced' and 'null'\n",
    "test_model = 'SIRwhiteN'                                     #test_models are 'SIRwhiteN', 'SIRenvN', 'SIRdemN', 'SEIR', and 'COVID'\n",
    "NoOFSim = 10                                                #Number of simulation for each type; 7 for COVID; 10 for others\n",
    "\n",
    "for simulation in np.arange(1, NoOFSim+1):\n",
    "    \n",
    "    # Filepath to residual time series to make predictions on \n",
    "    filepath = '../data/resids/resids_{}_{}{}.csv'.format(test_model, type, simulation)\n",
    "\n",
    "    # Filepath to export ensemble DL predictions to\n",
    "    filepath_out = '../data/ml_pred_Bury/ensemble_trend_probs_Bury_{}{}_{}_len500.csv'.format(test_model, type, simulation)\n",
    "    \n",
    "    # Type of classifier to use (1500 or 500)\n",
    "    ts_len=500   \n",
    "\n",
    "    '''  \n",
    "    The following two parameters control how many sample points along the \n",
    "    timeseries are used, and the length between them.  For instance, for an input \n",
    "    time series equal to or less then length 1500, mult_factor=10 and \n",
    "    pad_samples=150 means that it will do the unveiling in steps of 10 datapoints, \n",
    "    at 150 evenly spaced points across the entire time series (150 x 10 = 1500).\n",
    "    Needs to be adjusted according to whether you are using the trained 500 length \n",
    "    or 1500 length classifier.\n",
    "    '''\n",
    "\n",
    "    # Steps of datapoints in between each DL prediction\n",
    "    mult_factor = 10\n",
    "\n",
    "    # Total number of DL predictions to make\n",
    "    # Use 150 for length 1500 time series. Use 50 for length 500 time series.\n",
    "    pad_samples = 50\n",
    "\n",
    "    \n",
    "    df = pd.read_csv(filepath).dropna()\n",
    "    # Length of inupt time series \n",
    "    df_len = len(df)   \n",
    "\n",
    "    if df_len > ts_len:\n",
    "        df_last500 = df[-500:]              #df of last 500 points \n",
    "        resids = df_last500['residuals'].values.reshape(1,-1,1)\n",
    "        input_series = resids\n",
    "        seq_len = len(df_last500)\n",
    "    else:\n",
    "        resids = df['residuals'].values.reshape(1,-1,1)           #convert the input residue in an array of same length\n",
    "        # Length of inupt time series \n",
    "        seq_len = len(df)               \n",
    "        input_series = resids\n",
    "\n",
    "    def get_dl_predictions(resids, model_type, kk):\n",
    "\n",
    "        '''\n",
    "        Generate DL prediction time series on resids\n",
    "        from DL classifier with type 'model_type' and index kk.\n",
    "        '''\n",
    "\n",
    "        # Setup file to store DL predictions\n",
    "        predictions_file_name = 'predictions/y_pred_{}_{}.csv'.format(kk,model_type)\n",
    "        f1 = open(predictions_file_name,'w')\n",
    "\n",
    "        # Load in specific DL classifier\n",
    "        model_name = 'trained_models_Bury_et_al/best_model_{}_{}_len{}.pkl'.format(kk,model_type,ts_len)\n",
    "        model = load_model(model_name)\n",
    "\n",
    "        # Loop through each possible length of padding\n",
    "        # Start with revelaing the DL algorith only the earliest points\n",
    "        for pad_count in range(pad_samples-1, -1, -1):\n",
    "\n",
    "            temp_ts = np.zeros((1,ts_len,1))\n",
    "\n",
    "            ts_gap = ts_len-seq_len\n",
    "            pad_length = mult_factor*pad_count\n",
    "\n",
    "            if pad_length + ts_gap > ts_len:\n",
    "                zero_range = ts_len\n",
    "            else:\n",
    "                zero_range = pad_length + ts_gap\n",
    "\n",
    "            if zero_range == ts_len:\n",
    "                # Set all ML predictions to zero\n",
    "                y_pred = np.zeros(4).reshape(1,4)\n",
    "            else:\n",
    "                for j in range(0, zero_range):\n",
    "                    temp_ts[0,j] = 0\n",
    "                for j in range(zero_range, ts_len):\n",
    "                    temp_ts[0,j] = resids[0,j-zero_range]\n",
    "\n",
    "                # normalizing inputs: take averages, since the models were also trained on averaged data. \n",
    "                values_avg = 0.0\n",
    "                count_avg = 0\n",
    "                for j in range (0,ts_len):\n",
    "                    if temp_ts[0,j] != 0:\n",
    "                        values_avg = values_avg + abs(temp_ts[0,j])\n",
    "                        count_avg = count_avg + 1\n",
    "                if count_avg != 0:\n",
    "                    values_avg = values_avg/count_avg\n",
    "                for j in range (0,ts_len):\n",
    "                    if temp_ts[0,j] != 0:\n",
    "                        temp_ts[0,j] = temp_ts[0,j]/values_avg\n",
    "\n",
    "                # Compute DL prediction\n",
    "                y_pred = model.predict(temp_ts)\n",
    "\n",
    "\n",
    "\n",
    "            # Write predictions to file\n",
    "            np.savetxt(f1, y_pred, delimiter=',')\n",
    "            print('Predictions computed for padding={}'.format(pad_count*mult_factor))\n",
    "\n",
    "        # Delete model and do garbage collection to free up RAM\n",
    "        tf.keras.backend.clear_session()\n",
    "        if zero_range != ts_len:\n",
    "            del model\n",
    "        gc.collect()\n",
    "        f1.close()\n",
    "\n",
    "        return \n",
    "\n",
    "\n",
    "\n",
    "    # Compute DL predictions from all 20 trained models\n",
    "    for model_type in [1,2]:                                \n",
    "        for kk in np.arange(1,11):\n",
    "            print('Compute DL predictions for model_type={}, kk={}'.format(\n",
    "                model_type,kk))\n",
    "\n",
    "            get_dl_predictions(resids, model_type, kk)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Compute average prediction among all 20 DL classifiers\n",
    "    list_df_preds = []\n",
    "    for model_type in [1,2]:\n",
    "        for kk in np.arange(1,11):\n",
    "            filename = 'predictions/y_pred_{}_{}.csv'.format(kk,model_type)\n",
    "            df_preds = pd.read_csv(filename,header=None)\n",
    "            df_preds['time_index'] = df_preds.index\n",
    "            df_preds['model_type'] = model_type\n",
    "            df_preds['kk'] = kk\n",
    "            list_df_preds.append(df_preds)\n",
    "\n",
    "\n",
    "    # Concatenate\n",
    "    df_preds_all = pd.concat(list_df_preds).reset_index(drop=True)\n",
    "\n",
    "    # Compute mean over all predictions\n",
    "    df_preds_mean = df_preds_all.groupby('time_index').mean()\n",
    "    df_preds_mean = df_preds_mean[[0,1,2,3]]\n",
    "    df_preds_mean = df_preds_mean.assign(b=df_preds_mean.iloc[:,[0,1,2]].sum(axis=1))\n",
    "\n",
    "    # Export predictions\n",
    "    df_preds_mean.iloc[:,[0,1,2,3,4]].to_csv(filepath_out,index=False,header=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
