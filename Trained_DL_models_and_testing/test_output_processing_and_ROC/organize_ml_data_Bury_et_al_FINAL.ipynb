{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "KoklnM1wDrbw",
   "metadata": {
    "id": "KoklnM1wDrbw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Organise ML data output by Bury et al trained model into a single dataframe. \n",
    "Edited from published codes by Bury et al. (2021), Deep learning for early warning signals of tipping points, PNAS. \n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "# length of classifier\n",
    "classifier_length=500\n",
    "# Spacing between ML data points\n",
    "ml_spacing = int(classifier_length/50)\n",
    "\n",
    "#Change your test model\n",
    "#test_models are: 'SIRwhiteN', 'SIRenvN', 'SIRdemN', 'SEIR'\n",
    "test_model = 'SIRwhiteN' \n",
    "\n",
    "# Import EWS data for variable x (required for time values of original data)\n",
    "df_ews = pd.read_csv('../data/ews/df_ews_forced_{}.csv'.format(test_model))\n",
    "df_ews = df_ews[df_ews['Variable']=='I']\n",
    "\n",
    "# Get all file names of ML predictions\n",
    "all_files = os.listdir('../data/ml_pred_Bury/')\n",
    "all_files = [f for f in all_files if f.split('_')[0]=='ensemble']\n",
    "\n",
    "# Get null and forced filenames\n",
    "all_files_null = [s for s in all_files if s.find(test_model + 'null')!=-1]\n",
    "all_files_forced = [s for s in all_files if s.find(test_model + 'forced')!=-1]\n",
    "\n",
    "\n",
    "\n",
    "#----------------\n",
    "# Organise data for forced trajectories\n",
    "#-----------------\n",
    "\n",
    "list_df_ml = []\n",
    "for filename in all_files_forced:\n",
    "    df = pd.read_csv('../data/ml_pred_Bury/{}'.format(filename),\n",
    "                     header = None,\n",
    "                     names = ['fold_prob','hopf_prob','branch_prob','null_prob','bif_prob'])\n",
    "\n",
    "    # Get tsid from filename\n",
    "    filename_split = filename.split('_')\n",
    "    tsid = int(filename_split[-2])\n",
    "    df['tsid'] = tsid\n",
    "\n",
    "\n",
    "    # Get time values up to the transition point\n",
    "    tVals = df_ews[(df_ews['tsid']==tsid)][['Time','residuals']].dropna()['Time'].values\n",
    "\n",
    "    # Take last 'classifier_length' time points of data\n",
    "    tValsLast = tVals[-classifier_length:]\n",
    "    # If shorter than classifier_length points, pad with Nan (this is done prior to using ML)\n",
    "    if len(tValsLast)<classifier_length:\n",
    "        tValsLast = np.pad(tValsLast, (classifier_length-len(tValsLast),0), constant_values=np.nan)\n",
    "    # ML time points spacing\n",
    "    ml_time_vals = tValsLast[::ml_spacing]\n",
    "\n",
    "    # Assign to df\n",
    "    df['Time']=ml_time_vals\n",
    "\n",
    "    # Append dataframe to list\n",
    "    list_df_ml.append(df)\n",
    "\n",
    "\n",
    "# Concatenate dfs\n",
    "df_ml = pd.concat(list_df_ml)\n",
    "# sort by type, then latitude\n",
    "df_ml.sort_values(['tsid','Time'],inplace=True)\n",
    "\n",
    "# Export ML dataframe\n",
    "df_ml.to_csv('../data/ml_pred_Bury/df_ml_forced_Bury_{}.csv'.format(test_model), index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#----------------\n",
    "# Organise data for null trajectories\n",
    "#-----------------\n",
    "\n",
    "list_df_ml = []\n",
    "for filename in all_files_null:\n",
    "    df = pd.read_csv('../data/ml_pred_Bury/{}'.format(filename),\n",
    "                     header = None,\n",
    "                     names = ['fold_prob','hopf_prob','branch_prob','null_prob','bif_prob'])\n",
    "\n",
    "    # Get tsid from filename\n",
    "    filename_split = filename.split('_')\n",
    "    tsid = int(filename_split[-2])\n",
    "    df['tsid'] = tsid\n",
    "\n",
    "\n",
    "    # Get time values for this transition\n",
    "    tVals = df_ews[(df_ews['tsid']==tsid)]['Time'].values\n",
    "\n",
    "    # Take last 'classifier_length' time points of data\n",
    "    tValsLast = tVals[-classifier_length:]\n",
    "    # If shorter than classifier_length points, pad with Nan (this is done prior to using ML)\n",
    "    if len(tValsLast)<classifier_length:\n",
    "        tValsLast = np.pad(tValsLast, (classifier_length-len(tValsLast),0), constant_values=np.nan)\n",
    "    # ML time points spacing\n",
    "    ml_time_vals = tValsLast[::ml_spacing]\n",
    "\n",
    "    # Assign to df\n",
    "    df['Time']=ml_time_vals\n",
    "\n",
    "    # Append dataframe to list\n",
    "    list_df_ml.append(df)\n",
    "\n",
    "\n",
    "# Concatenate dfs\n",
    "df_ml = pd.concat(list_df_ml)\n",
    "# sort by type, then latitude\n",
    "df_ml.sort_values(['tsid','Time'],inplace=True)\n",
    "\n",
    "# Export ML dataframe\n",
    "df_ml.to_csv('../data/ml_pred_Bury/df_ml_null_Bury_{}.csv'.format(test_model), index=False)\n",
    "\n",
    "print('Done')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
